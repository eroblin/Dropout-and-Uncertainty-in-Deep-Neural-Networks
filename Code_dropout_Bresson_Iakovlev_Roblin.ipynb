{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Réseau de Neurones et Dropout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## I. Classification  avec les données MNIST"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous nous appuyons dans un premier temps sur la base \"MNIST\" pour mettre en oeuvre l'utilisation du dropout dans le cas de la classification. Cette base est elle-même utilisée par les auteurs dans leur article. Cette base est constituée d'images représentant les chiffres de 1 à 10. L'objectif est de classifier les différentes images selon leur appartenance à ces dix chiffres. Pour des raisons de mémoire et pour simplifier la tâche ici, nous créons une sous base comprenant seulement les chiffres 0 et 1 en proportions égales. Nous constituons une base de test constituée de 1000 images et une base d'apprentissage composée de 2000 images. Nous définissons un réseau de neurones à deux couches cachées. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Les données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Téléchargement des données\n",
    "from sklearn.datasets import fetch_mldata\n",
    "custom_data_home = \"/cours/statistique_bayesienne/data\"\n",
    "mnist = fetch_mldata('MNIST original', data_home=custom_data_home)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Téléchargement des packages\n",
    "import numpy as np\n",
    "import sklearn.datasets\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# séparation des données\n",
    "X = mnist.data\n",
    "Y = mnist.target\n",
    "random_x=[i for i in range(1000)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# base d'apprentissage/base de test\n",
    "X_train = X[:1000, ]\n",
    "Y_train = Y[:1000, ]\n",
    "X_train_bis = X[7000:8000, ]\n",
    "Y_train_bis = Y[7000:8000, ]\n",
    "X_test = X[5500:6500, ]\n",
    "Y_test = Y[5500:6500, ]\n",
    "X_tr = np.concatenate((X_train, X_train_bis))\n",
    "Y_tr = np.concatenate((Y_train, Y_train_bis))\n",
    "X_tr = X_tr.astype(int)\n",
    "Y_tr = Y_tr.astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({0.0: 423, 1.0: 577})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "Counter(Y_tr)\n",
    "Counter(Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Définition de la probabilité de dropout\n",
    "dropout_percent = 0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implémentation du réseau de neurones FNN deux couches cachées"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Définition de la fonction de coût, il s'agit de la fonction à minimser avec la méthode de descente de gradient\n",
    "def calculate_loss(model, X, y):\n",
    "    w1, b1, w2, b2, w3, b3 = model['w1'], model['b1'], model['w2'], model['b2'], model['w3'], model['b3']\n",
    "    # Forward propagation to calculate our predictions\n",
    "    z1 = X.dot(w1) + b1\n",
    "    l1 = np.tanh(z1)\n",
    "    z2 = l1.dot(w2) + b2\n",
    "    l2 = np.tanh(z2)\n",
    "    z3 = l2.dot(w3) + b3\n",
    "    exp_scores = np.exp(z3)\n",
    "    probs = exp_scores / np.sum(exp_scores, axis=1, keepdims=True)\n",
    "\n",
    "    corect_logprobs = -np.log(probs[range(num_x), y])\n",
    "    data_loss = np.sum(corect_logprobs)\n",
    "    data_loss += reg_lambda / 6 * (np.sum(np.square(w1)) + np.sum(np.square(w2)) + np.sum(np.square(w3)) + np.sum(np.square(b1))\n",
    "                                   + np.sum(np.square(b2)) + np.sum(np.square(b3)))\n",
    "\n",
    "    return 1. / num_x * data_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fonction construisant le modèle\n",
    "def build_model(hdim_1, hdim_2, num_it, X, y):\n",
    "\n",
    "    # Initialisation aléatoire des paramètres\n",
    "    np.random.seed(0)\n",
    "    w1 = np.random.randn(input_dim, hdim_1) / np.sqrt(input_dim)\n",
    "    b1 = np.zeros((1, hdim_1))\n",
    "    w2 = np.random.randn(hdim_1, hdim_2) / np.sqrt(hdim_1)\n",
    "    b2 = np.zeros((1, hdim_2))\n",
    "    w3 = np.random.randn(hdim_2, output_dim) / np.sqrt(hdim_2)\n",
    "    b3 = np.zeros((1, output_dim))\n",
    "\n",
    "    model = {}\n",
    "\n",
    "    for i in range(0, num_it):\n",
    "\n",
    "        # Forward propagation\n",
    "        z1 = X.dot(w1) + b1\n",
    "        l1 = np.tanh(z1)\n",
    "        z2 = l1.dot(w2) + b2\n",
    "        l2 = np.tanh(z1)\n",
    "        l2 *= np.random.binomial([np.ones((len(X), hdim_2))],\n",
    "                                 1 - dropout_percent)[0] * (1.0 / (1 - dropout_percent))\n",
    "        z3 = l2.dot(w3) + b3\n",
    "        exp_scores = np.exp(z3)\n",
    "        probs = exp_scores / np.sum(exp_scores, axis=1, keepdims=True)\n",
    "\n",
    "        # Backpropagation\n",
    "        delta4 = probs\n",
    "        delta4[range(num_x), y] -= 1\n",
    "        dw3 = (l2.T).dot(delta4)\n",
    "        db3 = np.sum(delta4, axis=0, keepdims=True)\n",
    "        delta3 = delta4.dot(w3.T) * (1 - np.power(l2, 2))\n",
    "        dw2 = (l1.T).dot(delta3)\n",
    "        db2 = np.sum(delta3, axis=0, keepdims=True)\n",
    "        delta2 = delta3.dot(w2.T) * (1 - np.power(l1, 2))\n",
    "        dw1 = np.dot(X.T, delta2)\n",
    "        db1 = np.sum(delta2, axis=0)\n",
    "\n",
    "        # Terme de régularisation\n",
    "        dw2 += reg_lambda / 3 * w2\n",
    "        dw1 += reg_lambda / 3 * w1\n",
    "        dw3 += reg_lambda / 3 * w3\n",
    "        db1 += reg_lambda / 3 * b1[:, 0]\n",
    "        db2 += reg_lambda / 3 * b2\n",
    "        db3 += reg_lambda / 3 * b3\n",
    "\n",
    "        #  Mise à jour des paramètres dans la descente de gradient\n",
    "        w1 += -epsilon * dw1\n",
    "        b1 += -epsilon * db1\n",
    "        w2 += -epsilon * dw2\n",
    "        b2 += -epsilon * db2\n",
    "        w3 += -epsilon * dw3\n",
    "        b3 += -epsilon * db3\n",
    "\n",
    "        model = {'w1': w1, 'b1': b1, 'w2': w2, 'b2': b2, 'w3': w3, 'b3': b3}\n",
    "\n",
    "        # Calcul de l'erreur\n",
    "        if i % 1000 == 0:\n",
    "            print(\"Loss after iteration %i: %f\" %\n",
    "                  (i, calculate_loss(model, X, y)))\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss after iteration 0: 0.425489\n",
      "Loss after iteration 1000: 0.059366\n",
      "Loss after iteration 2000: 0.134281\n",
      "Loss after iteration 3000: 0.308279\n",
      "Loss after iteration 4000: 0.574985\n",
      "Loss after iteration 5000: 0.948007\n",
      "Loss after iteration 6000: 1.415561\n",
      "Loss after iteration 7000: 1.980560\n",
      "Loss after iteration 8000: 2.644123\n",
      "Loss after iteration 9000: 3.413304\n"
     ]
    }
   ],
   "source": [
    "# Apprentissage du modèle pour des paramètres données, deux couches cachées de 20 neurones, 10000 itérations\n",
    "num_x = len(X_tr)\n",
    "input_dim = 784\n",
    "output_dim = 2\n",
    "epsilon = 0.001\n",
    "reg_lambda = 0.005\n",
    "model = build_model(20, 20, 10000, X_tr, Y_tr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Base de test et dropout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On effectue alors le dropout 100 fois en utilisant une loi binomiale appliquée aux poids de la dernière couche cachée. La loi utilisée pour effectuée le dropout est la loi de Bernoulli."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fonction lançant le dropout à utiliser sur les observations de la base de test\n",
    "def dropout(model, x, dropout_percent):\n",
    "    w1, b1, w2, b2, w3, b3 = model['w1'], model['b1'], model['w2'], model['b2'], model['w3'], model['b3']\n",
    "    # Forward propagation\n",
    "    z1 = x.dot(w1) + b1\n",
    "    l1 = np.tanh(z1)\n",
    "    z2 = l1.dot(w2) + b2\n",
    "    l2 = np.tanh(z1)\n",
    "    # Mise en place du dropout avec binomiale\n",
    "    l2 *= np.random.binomial([np.ones((len(x), hdim_1))],\n",
    "                             1 - dropout_percent)[0] * (1.0 / (1 - dropout_percent))\n",
    "    z3 = l2.dot(w3) + b3\n",
    "    exp_scores = np.exp(z3)\n",
    "    probs = exp_scores / np.sum(exp_scores, axis=1, keepdims=True)\n",
    "    #\"the model uncertainty in the softmax output can be summarised by taking the mean of the distribution\"\n",
    "    return probs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On teste avec une probabilité de dropout de 0.02. On effectue 100 dropout aléatoires sur le modèle pour obtenir l'incertitude du modèle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prédictions des étiquettes (1 ou 0) pour les 1000 observations de la base de test en utilisant la méthode dropout\n",
    "# predictive_mean contient les probabilités finales, et predictive_variance contient la variance de la loi a posteriori\n",
    "hdim_1 = 20\n",
    "hdim_2 = 20\n",
    "probs_tot = []\n",
    "T = 100\n",
    "dropout_percent = 0.5\n",
    "l = 2\n",
    "for _ in range(T):\n",
    "    probs = dropout(model, X_test, dropout_percent)\n",
    "    probs_tot += [probs.T[1]]\n",
    "\n",
    "    predictive_mean = np.mean(probs_tot, axis=0)\n",
    "predictive_variance = np.var(probs_tot, axis=0)\n",
    "tau = l**2 * (1 - dropout_percent) / (2 * len(X_test) * reg_lambda)\n",
    "predictive_variance += tau**-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# on transforme predictive_mean en valeurs d'étiquettes 0 ou 1\n",
    "predictive_class = [1 if i >= 0.5 else 0 for i in predictive_mean]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section dédiée à la visualisation des résultats et de l'incertitude"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importation des packages nécessaires\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import plotly\n",
    "from plotly.graph_objs import Scatter, Layout\n",
    "import plotly.graph_objs as go"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[420,   3],\n",
       "       [  0, 577]], dtype=int64)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# matrice de confusion des résultats\n",
    "m = confusion_matrix(Y_test, predictive_class, labels=None, sample_weight=None)\n",
    "m"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Petite transformation décimale des résultats pour pouvoir les visualiser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "u = np.array(probs_tot).T\n",
    "for i in range(1000):\n",
    "    for j in range(100):\n",
    "        u[i, j] = int(u[i, j] * 1000) / 1000\n",
    "\n",
    "u1 = np.concatenate((u, np.array(predictive_class, ndmin=2).T), axis=1)\n",
    "u1 = u1[u1[:, 100].argsort()]\n",
    "# u1 contient les probabilités prédites pour chaque boucle sur le dropout mais également les étiquettes prédites\n",
    "# triées pour une meilleure visualisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'file://C:\\\\Users\\\\asus\\\\Documents\\\\ENSAE\\\\3A\\\\3A ENSAE\\\\Stat Bay\\\\Code_readme_Bresson_Iakovlev_Roblin (2)\\\\temp-plot.html'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# graphiques permettant de visualiser l'incertitude sur les résultats (seulement 50 prédictions affichées par commodité)\n",
    "\n",
    "data = []\n",
    "\n",
    "for i in range(50):\n",
    "    trace0 = go.Scatter(\n",
    "        x=random_x,\n",
    "        y=u1[:, i],\n",
    "        mode='markers',\n",
    "        name='markers',\n",
    "        marker=dict(\n",
    "            size=2,\n",
    "            color='rgba(255, 182, 193, .9)',\n",
    "            line=dict(\n",
    "                width=1,\n",
    "            )\n",
    "        ))\n",
    "\n",
    "    data.append(trace0)\n",
    "\n",
    "trace1 = go.Scatter(\n",
    "    x=random_x,\n",
    "    y=u1[:, 100],\n",
    "    mode='lines',\n",
    "    name='lines'\n",
    ")\n",
    "data.append(trace1)\n",
    "\n",
    "\n",
    "plotly.offline.plot(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Partie Modèle Dropout régresseur, nouvelle données à charger"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Les données étudiées ici sont des séries temporelles portant sur la consommation totale en électricité d'un foyer sur 5 ans. Nous cherchons à prédire la consommation en temps réel à l'aide de 5 variables explicatives représentant des mesures de tension, intensité etc... dans le foyer. Nous chercherons à visualiser l'incertitude de prédiction à l'aide d'un modèle Dropout."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\asus\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py:2728: DtypeWarning:\n",
      "\n",
      "Columns (2,3,4,5,6,7) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# chargement des données\n",
    "base = pd.read_csv('C:/Users/asus/Documents/ENSAE/3A/3A ENSAE/Stat Bay/household_power_consumption.txt', sep=';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# on enlève les lignes avec des nan\n",
    "base = base.dropna(axis=0, how='any')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# séparation base de test et base d'apprentissage\n",
    "\n",
    "Xreg_train = np.array(base)[:50000, 3:]\n",
    "Yreg_train = np.array(base)[:50000, 2]\n",
    "Xreg_test = np.array(base)[50001:75000, 3:]\n",
    "Yreg_test = np.array(base)[50001:75000, 2]\n",
    "Xreg_tr = Xreg_train.astype(float)\n",
    "Yreg_tr = np.array(Yreg_train.astype(float), ndmin=2).T\n",
    "\n",
    "Xreg_tt = Xreg_test.astype(float)\n",
    "Yreg_tt = Yreg_test.astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Création de la fonction d'activation Relu et de sa dérivée\n",
    "def ReLU(x):\n",
    "    return x * (x > 0)\n",
    "\n",
    "\n",
    "def dReLU(x):\n",
    "    if x > 0:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fonction de perte pour la régression\n",
    "def calculate_loss(model, X, y):\n",
    "    w1, b1, w3, b3 = model['w1'], model['b1'], model['w3'], model['b3']\n",
    "    # Forward propagation to calculate our predictions\n",
    "    z1 = X.dot(w1) + b1\n",
    "    l1 = np.array([ReLU(x) for x in z1])\n",
    "    z3 = l1.dot(w3) + b3\n",
    "    prediction = np.array([ReLU(x) for x in z3])\n",
    "    loss = 1 / len(prediction) * np.sum(np.square(prediction - y)) + reg_lambda / 4 * ((np.sum(np.square(w1)) + np.sum(np.square(w3))) +\n",
    "                                                                                       (np.sum(np.square(b1)) + +np.sum(np.square(b3))))\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# construction du modèle\n",
    "def build_model(hdim_1, num_it, X, y, dropout_percent):\n",
    "\n",
    "    # Initialisation des paramètres\n",
    "    np.random.seed(0)\n",
    "\n",
    "    w1 = np.random.randn(input_dim, hdim_1) / np.sqrt(input_dim)\n",
    "    b1 = np.zeros((X.shape[0], hdim_1))\n",
    "    w3 = np.random.randn(hdim_1, output_dim) / np.sqrt(hdim_1)\n",
    "    b3 = np.zeros((X.shape[0], output_dim))\n",
    "\n",
    "    model = {}\n",
    "\n",
    "    for i in range(0, num_it):\n",
    "\n",
    "        # Forward propagation\n",
    "        z1 = X.dot(w1) + b1\n",
    "        l1 = np.array([ReLU(x) for x in z1])\n",
    "        drops = np.random.binomial(\n",
    "            [np.ones((len(X), hdim_1))], 1 - dropout_percent)[0] * (1.0 / (1 - dropout_percent))\n",
    "        l1 *= drops\n",
    "        z3 = l1.dot(w3) + b3\n",
    "        prediction = z3\n",
    "\n",
    "        # Backprop to compute gradients of w1 and w3 with respect to loss\n",
    "        grad_y_pred = 2 * (prediction - y)  # the last layer's error\n",
    "        dw3 = l1.T.dot(grad_y_pred)\n",
    "        db3 = grad_y_pred\n",
    "        dw3 = dw3 / np.linalg.norm(dw3, ord=None)\n",
    "        grad_h_relu2 = grad_y_pred.dot(w3.T)  # the second laye's error\n",
    "        grad_h2 = grad_h_relu2.copy()\n",
    "        grad_h2[z1 < 0] = 0  # the derivate of ReLU\n",
    "        sigmaprim = np.array([[dReLU(z1[i, j])\n",
    "                               for j in range(20)]for i in range(50000)])\n",
    "        dw1 = X.T.dot(grad_h2)\n",
    "        dw1 = dw1 / np.linalg.norm(dw1, ord=None)\n",
    "        db1 = grad_h2\n",
    "\n",
    "        # Terme de régularisation\n",
    "\n",
    "        dw1 += reg_lambda / 4 * w1\n",
    "        dw3 += reg_lambda / 4 * w3\n",
    "        db1 += reg_lambda / 4 * b1\n",
    "        db3 += reg_lambda / 4 * b3\n",
    "\n",
    "        #  Mise à jour des paramètres dans la descente de gradient\n",
    "        w1 += -epsilon * dw1\n",
    "        b1 += -epsilon * db1\n",
    "        w3 += -epsilon * dw3\n",
    "        b3 += -epsilon * db3\n",
    "\n",
    "        model = {'w1': w1, 'b1': b1, 'w3': w3, 'b3': b3}\n",
    "\n",
    "        # Calcul de l'erreur\n",
    "        if i % 100 == 0:\n",
    "            print(\"Loss after iteration %i: %f\" %\n",
    "                  (i, calculate_loss(model, X, y)))\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss after iteration 0: 47.643259\n",
      "Loss after iteration 100: 22.421216\n",
      "Loss after iteration 200: 34.668802\n",
      "Loss after iteration 300: 33.411170\n",
      "Loss after iteration 400: 28.894118\n",
      "Loss after iteration 500: 26.419170\n",
      "Loss after iteration 600: 24.800779\n",
      "Loss after iteration 700: 23.869259\n",
      "Loss after iteration 800: 23.545728\n",
      "Loss after iteration 900: 23.665117\n"
     ]
    }
   ],
   "source": [
    "# implémentation du modèle\n",
    "num_x = len(Xreg_tr)\n",
    "input_dim = 6\n",
    "output_dim = 1\n",
    "epsilon = 0.001\n",
    "reg_lambda = 0.005\n",
    "dropout_percent = 0.05\n",
    "model = build_model(20, 1000, Xreg_tr, Yreg_tr, 0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# définition de la fonction dropout\n",
    "def dropout(model, X, dropout_percent):\n",
    "    w1, b1, w3, b3 = model['w1'], model['b1'], model['w3'], model['b3']\n",
    "    # Forward propagation\n",
    "    z1 = X.dot(w1) + b1[0:X.shape[0], ]\n",
    "    l1 = np.array([ReLU(x) for x in z1])\n",
    "    drops = np.random.binomial([np.ones((len(X), hdim_1))],\n",
    "                               1 - dropout_percent)[0] * (1.0 / (1 - dropout_percent))\n",
    "    l1 *= drops\n",
    "    z3 = l1.dot(w3) + b3[0:X.shape[0], ]\n",
    "    prediction = z3\n",
    "    #\"the model uncertainty in the softmax output can be summarised by taking the mean of the distribution\"\n",
    "    return prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# méthode dropout appliquée à la base de test sur 100 itérations\n",
    "hdim_1 = 20\n",
    "prediction_tot = []\n",
    "T = 100\n",
    "dropout_percent = 0.05\n",
    "l = 2\n",
    "for _ in range(T):\n",
    "    prediction = dropout(model, Xreg_tt, dropout_percent)\n",
    "    prediction_tot += [prediction.T]\n",
    "predictive_mean = np.mean(prediction_tot, axis=0)\n",
    "predictive_variance = np.var(prediction_tot, axis=0)\n",
    "tau = l**2 * (1 - dropout_percent) / (2 * len(Xreg_tt) * reg_lambda)\n",
    "predictive_variance += tau**-1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section dédiée à la visualisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transformation des données pour visualisation\n",
    "u = np.array(prediction_tot).T[:, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'file://C:\\\\Users\\\\asus\\\\Documents\\\\ENSAE\\\\3A\\\\3A ENSAE\\\\Stat Bay\\\\Code_readme_Bresson_Iakovlev_Roblin (2)\\\\temp-plot.html'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Premier graphique représentant la courbes des valeurs réelles et la courbes des valeurs prédites\n",
    "random_x = [i for i in range(1000)]\n",
    "\n",
    "# Create traces\n",
    "trace0 = go.Scatter(\n",
    "    x=random_x,\n",
    "    y=predictive_mean.T[0:1000, 0],\n",
    "    mode='lines',\n",
    "    name='Predictions',\n",
    "    marker=dict(\n",
    "        size=2,\n",
    "        color='blue',\n",
    "        line=dict(\n",
    "            width=1,\n",
    "        )\n",
    "    ))\n",
    "trace1 = go.Scatter(\n",
    "    x=random_x,\n",
    "    y=Yreg_tt[0:1000],\n",
    "    mode='lines',\n",
    "    name='True values',\n",
    "    marker=dict(\n",
    "        size=2,\n",
    "        color='grey',\n",
    "        line=dict(\n",
    "            width=1,\n",
    "        )\n",
    "    )\n",
    ")\n",
    "\n",
    "\n",
    "data = [trace0, trace1]\n",
    "plotly.offline.plot(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'file://C:\\\\Users\\\\asus\\\\Documents\\\\ENSAE\\\\3A\\\\3A ENSAE\\\\Stat Bay\\\\Code_readme_Bresson_Iakovlev_Roblin (2)\\\\temp-plot.html'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Second graphique représentant l'incertitude\n",
    "data = []\n",
    "\n",
    "for i in range(50):\n",
    "    trace0 = go.Scatter(\n",
    "        x=random_x,\n",
    "        y=u[0:1000, i],\n",
    "        mode='markers',\n",
    "        name='markers',\n",
    "        marker=dict(\n",
    "            size=2,\n",
    "            color='rgba(255, 182, 193, .9)',\n",
    "            line=dict(\n",
    "                width=1,\n",
    "            )\n",
    "        ))\n",
    "\n",
    "    data.append(trace0)\n",
    "\n",
    "trace1 = go.Scatter(\n",
    "    x=random_x,\n",
    "    y=predictive_mean.T[0:1000, 0],\n",
    "    mode='lines',\n",
    "    name='lines'\n",
    ")\n",
    "data.append(trace1)\n",
    "\n",
    "\n",
    "plotly.offline.plot(data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
